{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certificate in AI Study Guide\n",
    "=============================\n",
    "\n",
    "Scientific Computing\n",
    "--------------------\n",
    "\n",
    "### Equations\n",
    "\n",
    "* Derivatives\n",
    "* Partial Derivatives\n",
    "\n",
    "### Matrices\n",
    "\n",
    "#### Addition\n",
    "#### Substraction\n",
    "#### Multiplication\n",
    "#### Dot-Product\n",
    "#### Cross-Product\n",
    "#### Determinant\n",
    "[Determinant](https://en.wikipedia.org/wiki/Determinant): In linear algebra, the *determinant* is a value that can be computed from the elements of a square matrix. The determinant of a matrix A is denoted det(A), det A, or |A|. Geometrically, it can be viewed as the scaling factor of the linear transformation described by the matrix.<br/> ![2x2 Matrix Determinant](https://wikimedia.org/api/rest_v1/media/math/render/svg/5b2e40d390e1d26039aabee44c7d1d86c8755232) <br/> ![3X3 Matrix Determinant](https://wikimedia.org/api/rest_v1/media/math/render/svg/14f2f2a449d6d152ee71261e47551aa0a31c801e)\n",
    "#### Vector\n",
    "#### Transpose\n",
    "#### Inverse\n",
    "\n",
    "### Linear Algebra\n",
    "\n",
    "#### Principal Components Analysis (PCA)\n",
    "#### Eigenvectors &amp; Eigenvalues\n",
    "In linear algebra, an eigenvector or characteristic vector of a linear transformation is a non-zero vector that changes by only a scalar factor when that linear transformation is applied to it.\n",
    "Eigenvectors and eigenvalues have many important applications in computer vision and machine learning in general. Well known examples are PCA (Principal Component Analysis) for dimensionality reduction or EigenFaces for face recognition\n",
    "An eigenvector is a vector whose direction remains unchanged when a linear transformation is applied to it.\n",
    "[Read more..](http://www.visiondummy.com/2014/03/eigenvalues-eigenvectors/)\n",
    "\n",
    "### Probabilities\n",
    "\n",
    "* Random\n",
    "* Distribution\n",
    "* Visualisation/Plots\n",
    "\n",
    "\n",
    "Deep Neural Network\n",
    "-------------------\n",
    "\n",
    "### Theory\n",
    "\n",
    "#### Gradient Descent\n",
    "Stochastic gradient descent (often shortened to SGD), also known as incremental gradient descent, is an iterative method for optimizing a differentiable objective function, a stochastic approximation of gradient descent optimization.\n",
    "t is called stochastic because samples are selected randomly (or shuffled) instead of as a single group (as in standard gradient descent) or in the order they appear in the training set.\n",
    "It's the simplest way to operate the ANN is to feed in one sample at a time and apply back propagation, called stochastic gradient descent.\n",
    "* Activation\n",
    "* Bias\n",
    "* Weights\n",
    "* Nodes/Kernels\n",
    "* Forward / Bacward Propagation\n",
    "* **Epochs**: In a configured neural network model that can be trained, the number of epoch needs to be specified. Each epoch corresponds to one complete forward and backward propogation of the neural network (with the weights at each neuron being updated during back propogation).\n",
    "\n",
    "* Max-Pooling\n",
    "* Training and Test Sets\n",
    "* Dropout Layer\n",
    "* Mini-batch\n",
    "* Sigmoid/ReLU/Softmax\n",
    "\n",
    "### Various Neural Network\n",
    "\n",
    "https://machinelearningmastery.com/when-to-use-mlp-cnn-and-rnn-neural-networks/\n",
    "\n",
    "* Regression / Classification\n",
    "* RBM\n",
    "* ANN\n",
    "* **RNN**: One restriction of a Restricted Boltzmann Machine (RBM), is that the nodes within a layer do not connect to each other.\n",
    "A RNN breaks this rule by allowing to connect nodes from the same layer, allowing for dependencies between inputs.\n",
    "This makes the RNN suitable for timeseries or sequential data. \n",
    "![Modes](https://i.stack.imgur.com/b4sus.jpg)\n",
    "  * **One-to-one:** Dense() layer\n",
    "  * **One-to-many:** RepeatVector() + LTSM() layers (workaround as not supported in Keras)\n",
    "  * **Many-to-one:** Sequential()\n",
    "  * **Many-to-many:** LTSM()\n",
    "* CNN\n",
    "* GAN\n",
    "* Autoencoder\n",
    "* Decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
