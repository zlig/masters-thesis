{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certificate in AI Study Guide\n",
    "=============================\n",
    "\n",
    "Scientific Computing\n",
    "--------------------\n",
    "\n",
    "### Equations\n",
    "\n",
    "* Derivatives\n",
    "* Partial Derivatives\n",
    "\n",
    "### Matrices\n",
    "\n",
    "\n",
    "#### Addition\n",
    "\n",
    "\n",
    "#### Substraction\n",
    "\n",
    "\n",
    "#### Multiplication\n",
    "\n",
    "\n",
    "#### Dot-Product\n",
    "\n",
    "\n",
    "#### Cross-Product\n",
    "\n",
    "\n",    
    "#### Determinant\n",
    "[Determinant](https://en.wikipedia.org/wiki/Determinant): In linear algebra, the *determinant* is a value that can be computed from the elements of a square matrix. The determinant of a matrix A is denoted det(A), det A, or |A|. Geometrically, it can be viewed as the scaling factor of the linear transformation described by the matrix.<br/> ![2x2 Matrix Determinant](https://wikimedia.org/api/rest_v1/media/math/render/svg/5b2e40d390e1d26039aabee44c7d1d86c8755232) <br/> ![3X3 Matrix Determinant](https://wikimedia.org/api/rest_v1/media/math/render/svg/14f2f2a449d6d152ee71261e47551aa0a31c801e)\n",
    "\n",
    "#### Vector\n",
    "\n",
    "\n",
    "#### Transpose\n",
    "In linear algebra, the transpose of a matrix is an operator which flips a matrix over its diagonal, that is it switches the row and column indices of the matrix by producing another matrix denoted with a T in indice.\n",
    "![Matrix Transpose](https://upload.wikimedia.org/wikipedia/commons/e/e4/Matrix_transpose.gif)\n",
    "\n",
    "#### Inverse\n",
    "To calculate the inverse of a matrix, swap the positions of a and d, put negatives in front of b and c, and divide everything by the determinant \n",
    "![2x2 inverse](https://www.mathsisfun.com/algebra/images/matrix-inverse-2x2.svg)\n",
    "\n",
    "### Linear Algebra\n",
    "\n",
    "#### Principal Components Analysis (PCA)\n",
    "PCA is a dimension reduction technique.\n",
    "\n",
    "#### Eigenvectors &amp; Eigenvalues\n",
    "In linear algebra, an eigenvector or characteristic vector of a linear transformation is a non-zero vector that changes by only a scalar factor when that linear transformation is applied to it.\n",
    "Eigenvectors and eigenvalues have many important applications in computer vision and machine learning in general. Well known examples are PCA (Principal Component Analysis) for dimensionality reduction or EigenFaces for face recognition\n",
    "An eigenvector is a vector whose direction remains unchanged when a linear transformation is applied to it.\n",
    "[Read more..](http://www.visiondummy.com/2014/03/eigenvalues-eigenvectors/)\n",
    "\n",
    "### Probabilities\n",
    "\n",
    "* Random\n",
    "* Distribution\n",
    "* Visualisation/Plots\n",
    "\n",
    "\n",
    "Deep Neural Network\n",
    "-------------------\n",
    "There are multiple type of [neural networks](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/neural_networks.html).\n",
    "![Neural Networks](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/assets/neuralnetworks.png)\n",
    "\n",
    "### Theory\n",
    "\n",
    "#### Regression and classification\n",
    "There are 2 main type of machine learning problems: regression and classification.\n",
    "\n",
    "Regression consists in estimating a value (e.g. guessing the price of a house from its size), based on continuous variables\n",
    "One reqgression technique is linear regression, where the model calculates the best path of a straight line to fit a set of points, refining the slope and the intercept.\n",
    "\n",
    "Classification, where the values are discrete, consists on identifying a category (e.g. object type from an image).\n",
    "\n",
    "#### Supervised learning\n",
    "Like human learning, supervised learning is a classification technique in which a model attempts to identify a category, and a feedback is given in the form of pre-labelled data sets.\n",
    "The model is tested using a training set and then evaluated using a test set\n",
    "\n",
    "#### Unsupervised learning\n",
    "No labels are given in unsupervised learning, instead the algorithm must determine the classes itself.\n",
    "The model find common patterns within the input variables of a dataset.\n",
    "\n",
    "#### Gradient Descent\n",
    "Stochastic gradient descent (often shortened to SGD), also known as incremental gradient descent, is an iterative method for optimizing a differentiable objective function, a stochastic approximation of gradient descent optimization.\n",
    "t is called stochastic because samples are selected randomly (or shuffled) instead of as a single group (as in standard gradient descent) or in the order they appear in the training set.\n",
    "It's the simplest way to operate the ANN is to feed in one sample at a time and apply back propagation, called stochastic gradient descent.\n",
    "* Activation\n",
    "* Bias\n",
    "* Weights\n",
    "* Nodes/Kernels\n",
    "* Forward / Bacward Propagation\n",
    "* **Epochs**: In a configured neural network model that can be trained, the number of epoch needs to be specified. Each epoch corresponds to one complete forward and backward propogation of the neural network (with the weights at each neuron being updated during back propogation).\n",
    "\n",
    "* Max-Pooling\n",
    "* Training and Test Sets\n",
    "* Dropout Layer\n",
    "* Mini-batch\n",
    "* Sigmoid/ReLU/Softmax\n",
    "\n",
    "### Various Neural Network\n",
    "\n",
    "https://machinelearningmastery.com/when-to-use-mlp-cnn-and-rnn-neural-networks/\n",
    "\n",
    "#### RBM\n",
    "\n",
    "#### ANN\n",
    "Artificial Neural Network (ANN) can either perform supervised or unsupervised learning.",
    "\n",
    "The most basic type of ANN is a feed forward network, with nodes arranged in layers, and all node in a layer connected to every nodes in the next layer.\n",
    "\n",
    "\n",
    "#### RNN",
    "One restriction of a Restricted Boltzmann Machine (RBM), is that the nodes within a layer do not connect to each other.\n",
    "A RNN breaks this rule by allowing to connect nodes from the same layer, allowing for dependencies between inputs.\n",
    "This makes the RNN suitable for timeseries or sequential data. \n",
    "![Modes](https://i.stack.imgur.com/b4sus.jpg)\n",
    "  * **One-to-one:** Dense() layer\n",
    "  * **One-to-many:** RepeatVector() + LTSM() layers (workaround as not supported in Keras)\n",
    "  * **Many-to-one:** Sequential()\n",
    "  * **Many-to-many:** LTSM()\n",
    "\n",
    "#### CNN\n",
    "\n",
    "#### GAN\n",
    "\n",
    "#### Autoencoder\n",
    "\n",
    "#### Decision trees",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
