{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certificate in AI Study Guide\n",
    "=============================\n",
    "\n",
    "Scientific Computing\n",
    "--------------------\n",
    "\n",
    "\n",
    "### Equations\n",
    "\n",
    "* Derivatives\n",
    "* Partial Derivatives\n",
    "\n",
    "\n",
    "\n",
    "### Scalar, Vector, Matrix\n",
    "\n",
    "A matrix is simply a rectangular array of numbers, a vector is a row (or column) of a matrix, while a scalar is an element of matrix or a vector.\n",
    "\n",
    "![Scalar Vector Matrix](https://www.mathsisfun.com/algebra/images/scalar-vector-matrix.svg)\n",
    "\n",
    "### Matrices Operations\n",
    "\n",
    "\n",
    "#### Addition\n",
    "![Addition](https://cdn.kastatic.org/googleusercontent/1zwnERArTuwdXjBNj_s0PNa1oE58dMWqy_NTPUW2o0a2FtFbk1SAYRdHRTiLAR5FjEaN9-pdCqZscJ0qkPYiW8rk)\n",
    "\n",
    "#### Substraction\n",
    "![Substraction](https://www.mathsisfun.com/algebra/images/matrix-subtraction.gif)\n",
    "\n",
    "#### Multiplication (Dot-Product)\n",
    "![Dot-Product](https://www.mathsisfun.com/algebra/images/matrix-multiply-a.svg)\n",
    "\n",
    "#### Cross-Product\n",
    "![Cross-Product](https://qph.fs.quoracdn.net/main-qimg-28aa55880ce4876a36f5fc286ef054e2)\n",
    "\n",    
    "#### Determinant\n",
    "[Determinant](https://en.wikipedia.org/wiki/Determinant): In linear algebra, the *determinant* is a value that can be computed from the elements of a square matrix. The determinant of a matrix A is denoted det(A), det A, or |A|. Geometrically, it can be viewed as the scaling factor of the linear transformation described by the matrix.<br/> ![2x2 Matrix Determinant](https://wikimedia.org/api/rest_v1/media/math/render/svg/5b2e40d390e1d26039aabee44c7d1d86c8755232) <br/> ![3X3 Matrix Determinant](https://wikimedia.org/api/rest_v1/media/math/render/svg/14f2f2a449d6d152ee71261e47551aa0a31c801e)\n",
    "\n",
    "#### Transpose\n",
    "In linear algebra, the transpose of a matrix is an operator which flips a matrix over its diagonal, that is it switches the row and column indices of the matrix by producing another matrix denoted with a T in indice.\n",
    "![Matrix Transpose](https://upload.wikimedia.org/wikipedia/commons/e/e4/Matrix_transpose.gif)\n",
    "\n",
    "#### Inverse\n",
    "To calculate the inverse of a matrix, swap the positions of a and d, put negatives in front of b and c, and divide everything by the determinant \n",
    "![2x2 inverse](https://www.mathsisfun.com/algebra/images/matrix-inverse-2x2.svg)\n",
    "\n",
    "### Linear Algebra\n",
    "\n",
    "#### Principal Components Analysis (PCA)\n",
    "PCA is a dimension reduction technique.\n",
    "\n",
    "#### Eigenvectors &amp; Eigenvalues\n",
    "In linear algebra, an eigenvector or characteristic vector of a linear transformation is a non-zero vector that changes by only a scalar factor when that linear transformation is applied to it.\n",
    "Eigenvectors and eigenvalues have many important applications in computer vision and machine learning in general. Well known examples are PCA (Principal Component Analysis) for dimensionality reduction or EigenFaces for face recognition\n",
    "An eigenvector is a vector whose direction remains unchanged when a linear transformation is applied to it.\n",
    "[Read more..](http://www.visiondummy.com/2014/03/eigenvalues-eigenvectors/)\n",
    "\n",
    "### Probabilities\n",
    "\n",
    "* Random\n",
    "* Distribution\n",
    "* Visualisation/Plots\n",
    "\n",
    "\n",
    "Deep Learning\n",
    "-------------------\n",
    "### Machine Learning\n",
    "\n",
    "#### Regression and classification\n",
    "There are 2 main type of machine learning problems: regression and classification.\n",
    "\n",
    "Regression consists in estimating a value (e.g. guessing the price of a house from its size), based on continuous variables\n",
    "One reqgression technique is linear regression, where the model calculates the best path of a straight line to fit a set of points, refining the slope and the intercept.\n",
    "\n",
    "Classification, where the values are discrete, consists on identifying a category (e.g. object type from an image).\n",
    "\n",
    "#### Gradient Descent\n",
    "Stochastic gradient descent (often shortened to SGD), also known as incremental gradient descent, is an iterative method for optimizing a differentiable objective function, a stochastic approximation of gradient descent optimization.\n",
    "It is called stochastic because samples are selected randomly (or shuffled) instead of as a single group (as in standard gradient descent) or in the order they appear in the training set.\n",
    "It's the simplest way to operate the ANN is to feed in one sample at a time and apply back propagation.\n",
    "\n",
    "#### Supervised learning\n",
    "Like human learning, supervised learning is a classification technique in which a model attempts to identify a category, and a feedback is given in the form of pre-labelled data sets.\n",
    "The model is tested using a training set and then evaluated using a test set\n",
    "\n",
    "#### Unsupervised learning\n",
    "No labels are given in unsupervised learning, instead the algorithm must determine the classes itself.\n",
    "The model find common patterns within the input variables of a dataset.\n",
    "\n",
    "\n",
    "Neural Networks\n",
    "-------------------\n",
    "\n",
    "### Theory\n",
    "\n",
    "#### Model\n",
    "A model is simply a system for mapping inputs to outputs, which represents a theory about a problem. Models are useful because we can use them to predict the values of outputs for new data points given the inputs.\n",
    "\n",
    "#### Activation\n",
    "Activation functions are an extremely important feature of the artificial neural networks. They basically decide whether a neuron should be activated or not. Whether the information that the neuron is receiving is relevant for the given information or should it be ignored.\n",
    "\n",
    "#### Loss function\n",
    "Loss functions are a key part of any machine learning model: they define an objective against which the performance of your model is measured, and the setting of weight parameters learned by the model is determined by minimizing a chosen loss function. There are several different common loss functions to choose from: the cross-entropy loss, the mean-squared error, the huber loss, and the hinge loss â€“ just to name a few. (Rohan Varma)\n",
    "\n",
    "#### Bias\n",
    "Biases are values associated with each node in the input and hidden of a network, but in practice are treated in exactly the same manner as other weights. The use of biases in a neural network increases the capacity of the network to solve problems.\n",
    "\n",
    "#### Weights\n",
    "Neural networks are, as the name suggests, made up of neurons. These neurons tend to be remarkably simple, with nothing but a floating point value, an input, and an output. That float is what we refer to as the weight of a neuron.\n",
    "When a neural network is running, it takes a large data set, splits it into a bunch of tiny fragments, and disperses those fragments among all of the neurons contained within. The neurons take the data there receive, operate on it using the stored weight (the exact operation can vary, but generally you just multiply the value by the weight) and then pass on the resulting data to the output. At the end, all of the outputs are aggregated to come to a conclusion. If the network is still being trained, this conclusion will be evaluated for correctness, and then the weights of all the neurons involved will be adjusted slightly, reducing the values of the ones that were wrong and increasing the ones that were right.\n",
    "\n",
    "#### Nodes/Kernels\n",
    "In a Neural Network, the nodes contains the kernel functions. These kernel methods are a class of algorithms for pattern analysis, whose best known member is the support vector machine (SVM).\n",
    "\n",
    "#### Learning rate\n",
    "Simply speaking, the learning rate determines how fast weights (in case of a neural network) or the cooefficents (in case of linear regression or logistic regression) change.\n", 
    "The learning rate may not be a constant for all the layers of a neural network, it may be different for different layers which avoids problem of vanishing gradient i.e, weights may stop changing as weight change backpropogates itself to first layer, so a variable learning rate is assigned to each layer.\n",
    "\n",
    "The learning rate is one of the most important hyper-parameters to tune for training deep neural networks. ... Deep learning models are typically trained by a stochastic gradient descent optimizer. There are many variations of stochastic gradient descent: Adam, RMSProp, Adagrad..\n",
    "\n",
    "Gradient descent is like taking leaps in the current direction of the slope, and the learning rate is like the length of the leap you take.\n",
    "\n",
    "#### Activation function\n",
    "A neural network without an activation function is essentially just a linear regression model. The activation function does the non-linear transformation to the input making it capable to learn and perform more complex tasks, which would never be achievable usign lienar transformations.\n",
    "\n",
    "#### Overfitting\n",
    "Overfitting is one problem that can occur during the training of a neural network.\n",
    "Despite good results using the training set, the error rate is rather large when presented new test data.\n",
    "The network has memorized the training examples, but it has not learned to generalize to new situations.\n",
    "\n",
    "#### Underfitting\n",
    "Underfitting refers to a model that can neither model the training data nor generalize to new data. An underfit machine learning model is not a suitable model and will be obvious as it will have poor performance on the training data.\n",
    "A model that is underfit will have high training and high testing error while an overfit model will have extremely low training error but a high testing error.\n",
    "\n",
    "#### Regularisation\n",
    "\n",
    "\n",
    "#### Forward / Backward Propagation\n",
    "\n",
    "\n",
    "#### Epochs\n",
    "In a configured neural network model that can be trained, the number of epoch needs to be specified. Each epoch corresponds to one complete forward and backward propogation of the neural network (with the weights at each neuron being updated during back propogation).\n",
    "\n",
    "* Max-Pooling\n",
    "* Training and Test Sets\n",
    "* Dropout Layer\n",
    "* Mini-batch\n",
    "* Sigmoid/ReLU/Softmax\n",
    "\n",
    "\n",
    "### Various Neural Network\n",
    "\n",
    "https://machinelearningmastery.com/when-to-use-mlp-cnn-and-rnn-neural-networks/\n",
    "\n",
    "#### LTSM\n",
    "\n",
    "TODO [LTSM](https://nlpforhackers.io/keras-intro/)\n",
    "\n",
    "#### RBM\n",
    "\n",
    "#### Artificial Neural Network (ANN)\n",
    "An artificial neural network is an interconnected group of nodes (neurons), similar to the vast network of neurons in a brain. Each node represents an artificial neuron and a connection from the output of one artificial neuron goes into the input of another.\n",
    "\n",
    "Artificial Neural Network (ANN) can either perform supervised or unsupervised learning.",
    "\n",
    "The most basic type of ANN is a feed forward network, with nodes arranged in layers, and all node in a layer connected to every nodes in the next layer.\n",
    "\n",
    "#### RNN\n",
    "One restriction of a Restricted Boltzmann Machine (RBM), is that the nodes within a layer do not connect to each other.\n",
    "A RNN breaks this rule by allowing to connect nodes from the same layer, allowing for dependencies between inputs.\n",
    "This makes the RNN suitable for timeseries or sequential data. \n",
    "![Modes](https://i.stack.imgur.com/b4sus.jpg)\n",
    "  * **One-to-one:** Dense() layer\n",
    "  * **One-to-many:** RepeatVector() + LTSM() layers (workaround as not supported in Keras)\n",
    "  * **Many-to-one:** Sequential()\n",
    "  * **Many-to-many:** LTSM()\n",
    "\n",
    "#### CNN\n",
    "\n",
    "TODO [CNN](https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks-Part-2/)\n",
    "\n",
    "#### GAN\n",
    "\n",
    "#### Autoencoder\n",
    "\n",
    "#### Decision trees",
    "\n",
    "#### Overview\n",
    "There are multiple type of [neural networks](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/neural_networks.html).\n",
    "![Neural Networks](https://i.imgur.com/kJTR4V8.jpg)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
